{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12557980,"sourceType":"datasetVersion","datasetId":7482077}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import networkx as nx\nimport matplotlib.pyplot as plt\nimport re\nimport os # Import the os module for path manipulation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T02:39:43.611469Z","iopub.execute_input":"2025-08-07T02:39:43.612214Z","iopub.status.idle":"2025-08-07T02:39:43.876671Z","shell.execute_reply.started":"2025-08-07T02:39:43.612172Z","shell.execute_reply":"2025-08-07T02:39:43.875798Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Create the graph builder for applying graph algorithm","metadata":{}},{"cell_type":"code","source":"class GraphBuilder:\n    \"\"\"\n    A class to parse UCLA nodes and nets files, build a NetworkX graph,\n    and calculate graph centrality measures like Eigenvector Centrality and PageRank.\n    \"\"\"\n\n    def __init__(self, nodes_file_content: str, nets_file_content: str):\n        \"\"\"\n        Initializes the GraphBuilder with the content of UCLA nodes and nets files\n        and builds the internal graph.\n\n        Args:\n            nodes_file_content (str): The entire content of the .nodes file as a string.\n            nets_file_content (str): The entire content of the .nets file as a string.\n        \"\"\"\n        self._nodes_file_content = nodes_file_content\n        self._nets_file_content = nets_file_content\n        self.graph = self._build_graph_from_ispd_data()\n        print(\"GraphBuilder initialized and graph built.\")\n\n    def _parse_ucla_nodes(self) -> dict:\n        \"\"\"\n        Parses the content of a UCLA nodes file.\n\n        Returns:\n            dict: A dictionary mapping node IDs (e.g., 'o0') to None,\n                  just registering the node's existence.\n        \"\"\"\n        nodes = {}\n        lines = self._nodes_file_content.splitlines()\n        for line in lines:\n            line = line.strip()\n            # Check if the line starts with 'o' and has at least 3 parts (node_id, x, y/width, height)\n            # For simplicity, we only care about the node_id for graph construction.\n            if line.startswith('o') and len(line.split()) >= 3:\n                parts = line.split()\n                node_id = parts[0]\n                nodes[node_id] = None  # Just register the node's existence\n        return nodes\n\n    def _parse_ucla_nets(self) -> list:\n        \"\"\"\n        Parses the content of a UCLA nets file to extract connections between nodes.\n\n        Returns:\n            list: A list of lists, where each inner list contains node IDs\n                  connected by a single net.\n        \"\"\"\n        nets_connections = []\n        lines = self._nets_file_content.splitlines()\n        current_net_pins = []\n        in_net_block = False\n\n        for line in lines:\n            line = line.strip()\n            if line.startswith('NetDegree :'):\n                # If we were in a net block and collected pins, add them to connections\n                if in_net_block and current_net_pins:\n                    nets_connections.append(list(current_net_pins))\n                current_net_pins = [] # Reset for the new net\n                in_net_block = True\n                # Example: \"NetDegree : 4 n0\"\n                # We don't need 'n0' directly for graph edges, just the connected pins.\n            elif line.startswith('o') and in_net_block:\n                # Example: \"o197239 I : -0.500000 -6.000000\"\n                parts = line.split()\n                node_id = parts[0]\n                current_net_pins.append(node_id)\n            elif not line and in_net_block: # Empty line signals end of a net block (or almost)\n                if current_net_pins:\n                    nets_connections.append(list(current_net_pins))\n                current_net_pins = []\n                in_net_block = False\n            elif not line and not in_net_block: # Multiple empty lines\n                pass # Ignore\n            # This condition handles cases where a net block might not be followed by an empty line\n            # before the file ends or a new section begins.\n            elif not line.startswith('#') and not line.startswith('Num') and in_net_block:\n                # If we encounter a non-empty, non-comment, non-Num line while in a net block,\n                # it's likely the end of the previous net.\n                if current_net_pins:\n                    nets_connections.append(list(current_net_pins))\n                    current_net_pins = []\n                in_net_block = False # Exit net block state\n\n        # Append the last net's connections if loop ends within a net block\n        if current_net_pins:\n            nets_connections.append(list(current_net_pins))\n\n        return nets_connections\n\n    def _build_graph_from_ispd_data(self) -> nx.Graph:\n        \"\"\"\n        Builds a NetworkX graph from ISPD2005 nodes and nets file content\n        provided during initialization.\n\n        Returns:\n            nx.Graph: A graph representing the connections.\n        \"\"\"\n        print(\"Parsing nodes file...\")\n        nodes_data = self._parse_ucla_nodes()\n        print(f\"Found {len(nodes_data)} unique nodes.\")\n\n        print(\"Parsing nets file...\")\n        nets_connections = self._parse_ucla_nets()\n        print(f\"Found {len(nets_connections)} nets.\")\n\n        G = nx.Graph()\n\n        # Add all nodes first\n        for node_id in nodes_data:\n            G.add_node(node_id)\n\n        print(\"Adding edges based on net connections (clique model)...\")\n        # For each net, create edges between all connected nodes (clique model)\n        for net_pins in nets_connections:\n            if len(net_pins) > 1: # Only add edges if there are at least two pins on a net\n                # Add all possible pairs of nodes as edges for this net\n                for i in range(len(net_pins)):\n                    for j in range(i + 1, len(net_pins)):\n                        G.add_edge(net_pins[i], net_pins[j])\n        print(f\"Graph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n        return G\n\n    def calculate_eigenvector_centrality(self) -> list:\n        \"\"\"\n        Calculates eigenvector centrality for each node in the built graph and returns\n        the scores sorted in descending order.\n\n        Returns:\n            list: A list of tuples, where each tuple contains (node ID, eigenvector centrality score),\n                  sorted by score in descending order. Returns an empty list on error.\n        \"\"\"\n        print(\"Calculating Eigenvector Centrality...\")\n        try:\n            # Use max_iter and tol for convergence control\n            centrality = nx.eigenvector_centrality(self.graph, max_iter=1000, tol=1e-06)\n            # Sort the items by their centrality score in descending order\n            sorted_centrality = sorted(centrality.items(), key=lambda item: item[1], reverse=True)\n            return sorted_centrality\n        except nx.PowerIterationFailedConvergence:\n            print(\"Warning: Power iteration failed to converge. Trying numpy version for robustness.\")\n            try:\n                centrality = nx.eigenvector_centrality_numpy(self.graph)\n                sorted_centrality = sorted(centrality.items(), key=lambda item: item[1], reverse=True)\n                return sorted_centrality\n            except Exception as e:\n                print(f\"Error with eigenvector_centrality_numpy: {e}\")\n                return []\n        except Exception as e:\n            print(f\"An unexpected error occurred during centrality calculation: {e}\")\n            return []\n\n    def calculate_pagerank(self, alpha: float = 0.85, max_iter: int = 1000, tol: float = 1e-06) -> list:\n        \"\"\"\n        Calculates PageRank for each node in the built graph and returns\n        the scores sorted in descending order.\n\n        Args:\n            alpha (float): Damping factor (default is 0.85).\n            max_iter (int): Maximum number of iterations in power method.\n            tol (float): Tolerance for convergence.\n\n        Returns:\n            list: A list of tuples, where each tuple contains (node ID, PageRank score),\n                  sorted by score in descending order. Returns an empty list on error.\n        \"\"\"\n        print(f\"Calculating PageRank (alpha={alpha}, max_iter={max_iter}, tol={tol})...\")\n        try:\n            # NetworkX's pagerank can handle both directed and undirected graphs.\n            pagerank_scores = nx.pagerank(self.graph, alpha=alpha, max_iter=max_iter, tol=tol)\n            # Sort the items by their PageRank score in descending order\n            sorted_pagerank = sorted(pagerank_scores.items(), key=lambda item: item[1], reverse=True)\n            return sorted_pagerank\n        except nx.PowerIterationFailedConvergence:\n            print(\"Warning: PageRank power iteration failed to converge. Try increasing max_iter or adjusting alpha/tol.\")\n            return []\n        except Exception as e:\n            print(f\"An unexpected error occurred during PageRank calculation: {e}\")\n            return []\n\n    def calculate_degree_centrality(self) -> list:\n        \"\"\"\n        Calculates degree centrality for each node in the built graph and returns\n        the scores sorted in descending order.\n\n        Returns:\n            list: A list of tuples, where each tuple contains (node ID, degree centrality score),\n                  sorted by score in descending order. Returns an empty list on error.\n        \"\"\"\n        print(\"Calculating Degree Centrality...\")\n        try:\n            centrality = nx.degree_centrality(self.graph)\n            # Sort the items by their centrality score in descending order\n            sorted_centrality = sorted(centrality.items(), key=lambda item: item[1], reverse=True)\n            return sorted_centrality\n        except Exception as e:\n            print(f\"An unexpected error occurred during degree centrality calculation: {e}\")\n            return []\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T02:39:43.878664Z","iopub.execute_input":"2025-08-07T02:39:43.879247Z","iopub.status.idle":"2025-08-07T02:39:43.901930Z","shell.execute_reply.started":"2025-08-07T02:39:43.879219Z","shell.execute_reply":"2025-08-07T02:39:43.900609Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Testing GraphBuilder class","metadata":{}},{"cell_type":"code","source":"benchmark_name = '/kaggle/input/ispd2005/ispd2005/adaptec1' # <--- CHANGE THIS TO YOUR DESIRED BENCHMARK\nfolder_name = benchmark_name.split(\"/\")[-1]\nnodes_file_path = os.path.join(benchmark_name, f\"{folder_name}.nodes\")\nnets_file_path = os.path.join(benchmark_name, f'{folder_name}.nets')\n\nprint(f\"Attempting to read nodes from: {nodes_file_path}\")\nprint(f\"Attempting to read nets from: {nets_file_path}\")\n\nucla_nodes_content = \"\"\nucla_nets_content = \"\"\n\ntry:\n    with open(nodes_file_path, 'r') as f:\n        ucla_nodes_content = f.read()\n    with open(nets_file_path, 'r') as f:\n        ucla_nets_content = f.read()\nexcept FileNotFoundError:\n    print(f\"Error: Make sure the folder '{benchmark_name}' exists and contains '{folder_name}.nodes' and '{folder_name}.nets'.\")\n    exit()\nexcept Exception as e:\n    print(f\"An error occurred while reading files: {e}\")\n    exit()\ngraph_builder = GraphBuilder(ucla_nodes_content, ucla_nets_content)\ngraph = graph_builder.graph ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T02:39:43.902878Z","iopub.execute_input":"2025-08-07T02:39:43.903175Z","iopub.status.idle":"2025-08-07T02:39:58.646550Z","shell.execute_reply.started":"2025-08-07T02:39:43.903146Z","shell.execute_reply":"2025-08-07T02:39:58.645671Z"}},"outputs":[{"name":"stdout","text":"Attempting to read nodes from: /kaggle/input/ispd2005/ispd2005/adaptec1/adaptec1.nodes\nAttempting to read nets from: /kaggle/input/ispd2005/ispd2005/adaptec1/adaptec1.nets\nParsing nodes file...\nFound 211447 unique nodes.\nParsing nets file...\nFound 221142 nets.\nAdding edges based on net connections (clique model)...\nGraph created with 211447 nodes and 3909436 edges.\nGraphBuilder initialized and graph built.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"if graph.number_of_nodes() > 0:\n    # Call the updated method that returns sorted centrality\n    eigen_centrality_sorted = graph_builder.calculate_eigenvector_centrality()\n\n    if eigen_centrality_sorted:\n        print(f\"\\nEigenvector Centrality for {benchmark_name} benchmark:\")\n        print(\"Top 5 Nodes by Eigenvector Centrality:\")\n        for node, score in eigen_centrality_sorted[:5]:\n            print(f\"Node {node}: {score:.6f}\")\n\n        print(\"\\nBottom 5 Nodes by Eigenvector Centrality:\")\n        for node, score in eigen_centrality_sorted[-5:]:\n            print(f\"Node {node}: {score:.6f}\")\n\n        # Optional: Visualize a small subgraph for demonstration\n        if graph.number_of_nodes() < 50:\n            plt.figure(figsize=(10, 8))\n            pos = nx.spring_layout(graph, k=0.15, iterations=20)\n            # Recreate the dictionary for visualization if needed, or adjust node_sizes calculation\n            eigen_centrality_dict = {node: score for node, score in eigen_centrality_sorted}\n            node_sizes = [eigen_centrality_dict.get(node, 0) * 5000 for node in graph.nodes()]\n            nx.draw_networkx(graph, pos, with_labels=True, node_size=node_sizes,\n                             node_color='lightgreen', font_size=8, font_weight='bold',\n                             edge_color='gray', alpha=0.7)\n            plt.title(f\"{benchmark_name} Graph with Node Size Proportional to Eigenvector Centrality\")\n            plt.show()\n            print(f\"\\nGraph has {graph.number_of_nodes()} nodes. Skipping visualization for large graphs.\")\n    else:\n        print(\"Could not calculate eigenvector centrality.\")\nelse:\n    print(\"No nodes found to build a graph.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T02:39:58.648444Z","iopub.execute_input":"2025-08-07T02:39:58.648752Z","iopub.status.idle":"2025-08-07T02:40:11.810184Z","shell.execute_reply.started":"2025-08-07T02:39:58.648729Z","shell.execute_reply":"2025-08-07T02:40:11.809288Z"}},"outputs":[{"name":"stdout","text":"Calculating Eigenvector Centrality...\n\nEigenvector Centrality for /kaggle/input/ispd2005/ispd2005/adaptec1 benchmark:\nTop 5 Nodes by Eigenvector Centrality:\nNode o33313: 0.028305\nNode o210929: 0.028304\nNode o148188: 0.028301\nNode o132201: 0.028301\nNode o211444: 0.028301\n\nBottom 5 Nodes by Eigenvector Centrality:\nNode o203095: 0.000000\nNode o207695: 0.000000\nNode o205421: 0.000000\nNode o203023: 0.000000\nNode o203021: 0.000000\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"if graph.number_of_nodes() > 0:\n    # Call the updated method that returns sorted pagerank\n    pagerank_scores_sorted = graph_builder.calculate_pagerank(alpha=0.85, max_iter=1000, tol=1e-06)\n\n    if pagerank_scores_sorted:\n        print(f\"\\nPageRank Scores for {benchmark_name} benchmark:\")\n        print(\"Top 5 Nodes by PageRank:\")\n        for node, score in pagerank_scores_sorted[:5]:\n            print(f\"Node {node}: {score:.6f}\")\n\n        print(\"\\nBottom 5 Nodes by PageRank:\")\n        for node, score in pagerank_scores_sorted[-5:]:\n            print(f\"Node {node}: {score:.6f}\")\n\n        # Optional: Visualize a small subgraph for demonstration\n        if graph.number_of_nodes() < 50:\n            plt.figure(figsize=(10, 8))\n            pos = nx.spring_layout(graph, k=0.15, iterations=20)\n            # Recreate the dictionary for visualization if needed, or adjust node_sizes calculation\n            pagerank_scores_dict = {node: score for node, score in pagerank_scores_sorted}\n            node_sizes = [pagerank_scores_dict.get(node, 0) * 10000 for node in graph.nodes()] # Adjust multiplier for better visualization\n            nx.draw_networkx(graph, pos, with_labels=True, node_size=node_sizes,\n                             node_color='salmon', font_size=8, font_weight='bold',\n                             edge_color='gray', alpha=0.7)\n            plt.title(f\"{benchmark_name} Graph with Node Size Proportional to PageRank\")\n            plt.show()\n        else:\n            print(f\"\\nGraph has {graph.number_of_nodes()} nodes. Skipping visualization for large graphs.\")\n    else:\n        print(\"Could not calculate PageRank.\")\nelse:\n    print(\"No nodes found to build a graph.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T02:40:11.811245Z","iopub.execute_input":"2025-08-07T02:40:11.811504Z","iopub.status.idle":"2025-08-07T02:40:30.410209Z","shell.execute_reply.started":"2025-08-07T02:40:11.811482Z","shell.execute_reply":"2025-08-07T02:40:30.409036Z"}},"outputs":[{"name":"stdout","text":"Calculating PageRank (alpha=0.85, max_iter=1000, tol=1e-06)...\n\nPageRank Scores for /kaggle/input/ispd2005/ispd2005/adaptec1 benchmark:\nTop 5 Nodes by PageRank:\nNode o210925: 0.000198\nNode o210924: 0.000152\nNode o210908: 0.000137\nNode o210909: 0.000134\nNode o210906: 0.000133\n\nBottom 5 Nodes by PageRank:\nNode o201232: 0.000001\nNode o201236: 0.000001\nNode o201228: 0.000001\nNode o201245: 0.000001\nNode o201230: 0.000001\n\nGraph has 211447 nodes. Skipping visualization for large graphs.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"if graph.number_of_nodes() > 0:\n    degree_centrality_sorted = graph_builder.calculate_degree_centrality()\n\n    if degree_centrality_sorted:\n        print(f\"\\nDegree Centrality for {benchmark_name} benchmark:\")\n        print(\"Top 5 Nodes by Degree Centrality:\")\n        for node, score in degree_centrality_sorted[:5]:\n            print(f\"Node {node}: {score:.6f}\")\n\n        print(\"\\nBottom 5 Nodes by Degree Centrality:\")\n        for node, score in degree_centrality_sorted[-5:]:\n            print(f\"Node {node}: {score:.6f}\")\n\n        # Optional: Visualize a small subgraph for demonstration\n        if graph.number_of_nodes() < 50:\n            plt.figure(figsize=(10, 8))\n            pos = nx.spring_layout(graph, k=0.15, iterations=20)\n            # Recreate the dictionary for visualization\n            degree_centrality_dict = {node: score for node, score in degree_centrality_sorted}\n            node_sizes = [degree_centrality_dict.get(node, 0) * 5000 for node in graph.nodes()] # Adjust multiplier as needed\n            nx.draw_networkx(graph, pos, with_labels=True, node_size=node_sizes,\n                             node_color='skyblue', font_size=8, font_weight='bold',\n                             edge_color='gray', alpha=0.7)\n            plt.title(f\"{benchmark_name} Graph with Node Size Proportional to Degree Centrality\")\n            plt.show()\n        else:\n            print(f\"\\nGraph has {graph.number_of_nodes()} nodes. Skipping visualization for large graphs.\")\n    else:\n        print(\"Could not calculate degree centrality.\")\nelse:\n    print(\"No nodes found to build a graph.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T02:40:30.411494Z","iopub.execute_input":"2025-08-07T02:40:30.412723Z","iopub.status.idle":"2025-08-07T02:40:30.728816Z","shell.execute_reply.started":"2025-08-07T02:40:30.412680Z","shell.execute_reply":"2025-08-07T02:40:30.727912Z"}},"outputs":[{"name":"stdout","text":"Calculating Degree Centrality...\n\nDegree Centrality for /kaggle/input/ispd2005/ispd2005/adaptec1 benchmark:\nTop 5 Nodes by Degree Centrality:\nNode o210925: 0.011180\nNode o210924: 0.009700\nNode o210929: 0.008366\nNode o210919: 0.008196\nNode o33313: 0.008040\n\nBottom 5 Nodes by Degree Centrality:\nNode o210932: 0.000005\nNode o210933: 0.000005\nNode o210934: 0.000005\nNode o210935: 0.000005\nNode o211198: 0.000005\n\nGraph has 211447 nodes. Skipping visualization for large graphs.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Create new dataset from the GraphBuilder","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nfrom typing import List, Dict, Tuple\nimport shutil\nclass BenchmarkProcessor:\n    \"\"\"\n    A class to process all ISPD2005 benchmarks in a given directory,\n    rearranging the nodes and saving the new benchmark suite.\n    \"\"\"\n    def __init__(self, root_dir: str, method_name: str):\n        \"\"\"\n        Initializes the BenchmarkProcessor.\n\n        Args:\n            root_dir (str): The path to the root directory containing the benchmarks.\n            method_name (str): A string describing the rearrangement method. This\n                               will be used in the new folder names. Must be\n                               one of 'eigenvector', 'pagerank', or 'degree'.\n        \"\"\"\n        self.root_dir = Path(root_dir)\n        self.method_name = method_name.lower()\n        self.output_dir = self.root_dir.parent / f\"{self.root_dir.name}_{self.method_name}\"\n        self.centrality_methods = {\n            'eigenvector': 'calculate_eigenvector_centrality',\n            'pagerank': 'calculate_pagerank',\n            'degree': 'calculate_degree_centrality',\n        }\n        if self.method_name not in self.centrality_methods:\n            raise ValueError(f\"Invalid method name: {method_name}. Must be one of {list(self.centrality_methods.keys())}.\")\n\n    def _process_single_benchmark(self, benchmark_path: Path):\n        \"\"\"\n        Processes a single benchmark folder:\n        - Reads .nodes and .nets files.\n        - Calls the GraphBuilder class to calculate centrality.\n        - Creates a new folder for the modified benchmark.\n        - Writes the new .nodes file and copies the others.\n\n        Args:\n            benchmark_path (Path): The Path object for the benchmark directory.\n        \"\"\"\n        benchmark_name = benchmark_path.name\n        print(f\"\\nProcessing benchmark: {benchmark_name}\")\n\n        nodes_file = benchmark_path / f\"{benchmark_name}.nodes\"\n        nets_file = benchmark_path / f\"{benchmark_name}.nets\"\n        \n        # Check for required files\n        if not nodes_file.exists() or not nets_file.exists():\n            print(f\"Skipping {benchmark_name}: .nodes or .nets file not found.\")\n            return\n\n        # 1. Read file content\n        try:\n            with open(nodes_file, 'r') as f:\n                nodes_content = f.read()\n            with open(nets_file, 'r') as f:\n                nets_content = f.read()\n        except IOError as e:\n            print(f\"Error reading files for {benchmark_name}: {e}\")\n            return\n\n        # 2. Use GraphBuilder to get centrality scores\n        try:\n            graph_builder = GraphBuilder(nodes_content, nets_content)\n            method_to_call = getattr(graph_builder, self.centrality_methods[self.method_name])\n            centrality_scores = method_to_call()\n        except Exception as e:\n            print(f\"Error in GraphBuilder centrality calculation for {benchmark_name}: {e}\")\n            return\n\n        # 3. Create a new .nodes file content\n        new_nodes_content = self._generate_new_nodes_file(nodes_content, centrality_scores)\n\n        # 4. Create a new directory for the output\n        new_benchmark_name = f\"{benchmark_name}_{self.method_name}\"\n        new_benchmark_path = self.output_dir / new_benchmark_name\n        \n        if new_benchmark_path.exists():\n            shutil.rmtree(new_benchmark_path)\n        new_benchmark_path.mkdir(parents=True)\n        print(f\"Created new benchmark folder: {new_benchmark_path}\")\n        \n        # 5. Write the new .nodes file\n        new_nodes_file_path = new_benchmark_path / f\"{benchmark_name}.nodes\"\n        with open(new_nodes_file_path, 'w') as f:\n            f.write('\\n'.join(new_nodes_content))\n        print(f\"Saved new .nodes file to: {new_nodes_file_path}\")\n\n        # 6. Copy all other files\n        files_to_copy = [f for f in os.listdir(benchmark_path) if not f.endswith('.nodes')]\n        for file_name in files_to_copy:\n            shutil.copy(benchmark_path / file_name, new_benchmark_path / file_name)\n        print(f\"Copied {len(files_to_copy)} other files.\")\n\n    def _generate_new_nodes_file(self, original_nodes_content: str, scores: List[Tuple[str, float]]) -> List[str]:\n        \"\"\"\n        Generates the content for the new .nodes file, including the original\n        data and a comment block with the calculated centrality scores.\n        \"\"\"\n        original_lines = original_nodes_content.splitlines()\n        header = []\n        node_lines = []\n        for line in original_lines:\n            line = line.strip()\n            if line.startswith(('NumNodes', 'NumTerminals')):\n                header.append(line)\n            else:\n                node_lines.append(line)\n\n        modified_nodes_data = header\n        modified_nodes_data.extend(node_lines)\n        \n        modified_nodes_data.append(\"\\n# --- Centrality Scores ---\")\n        modified_nodes_data.append(f\"# Scores are calculated using the '{self.method_name}' method.\")\n        modified_nodes_data.append(\"# Format: node_id, score\\n\")\n\n        for node_id, score in scores:\n            modified_nodes_data.append(f\"# {node_id} {score}\")\n\n        return modified_nodes_data\n\n    def process_all_benchmarks(self):\n        \"\"\"\n        Runs through all valid benchmark folders in the root directory\n        and processes each one.\n        \"\"\"\n        if not self.root_dir.is_dir():\n            print(f\"Error: Directory not found at {self.root_dir}\")\n            return\n\n        # Ensure the output directory is ready\n        if self.output_dir.exists():\n            print(f\"Output directory '{self.output_dir}' already exists. Overwriting its contents.\")\n            shutil.rmtree(self.output_dir)\n        self.output_dir.mkdir(parents=True)\n\n        print(f\"Starting to process benchmarks from {self.root_dir}\")\n        for item in self.root_dir.iterdir():\n            if item.is_dir():\n                # Assumes each sub-directory is a benchmark\n                self._process_single_benchmark(item)\n        \n        print(\"\\nAll benchmarks processed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T02:40:30.730113Z","iopub.execute_input":"2025-08-07T02:40:30.730442Z","iopub.status.idle":"2025-08-07T02:40:30.747202Z","shell.execute_reply.started":"2025-08-07T02:40:30.730415Z","shell.execute_reply":"2025-08-07T02:40:30.746172Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# !cp -r /kaggle/input/ispd2005/ispd2005 /kaggle/working/ispd2005","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T02:40:30.748610Z","iopub.execute_input":"2025-08-07T02:40:30.749788Z","iopub.status.idle":"2025-08-07T02:40:30.771558Z","shell.execute_reply.started":"2025-08-07T02:40:30.749747Z","shell.execute_reply":"2025-08-07T02:40:30.770640Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# import shutil\n# import os\n\n# dir_path = '/kaggle/working/ispd2005_degree'  # Replace with your directory\n\n# if os.path.exists(dir_path):\n#     shutil.rmtree(dir_path)\n#     print(f\"Deleted: {dir_path}\")\n# else:\n#     print(f\"Directory does not exist: {dir_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T02:40:30.772562Z","iopub.execute_input":"2025-08-07T02:40:30.772918Z","iopub.status.idle":"2025-08-07T02:40:30.789574Z","shell.execute_reply.started":"2025-08-07T02:40:30.772892Z","shell.execute_reply":"2025-08-07T02:40:30.788400Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"benchmark_dir = \"/kaggle/working/ispd2005\"\nmethod = \"degree\"\ntry:\n    processor = BenchmarkProcessor(root_dir=benchmark_dir, method_name=method)\n    processor.process_all_benchmarks()\n    print(f\"\\nNew benchmark files are located in the '{processor.output_dir}' folder.\")\nexcept ValueError as e:\n    print(e)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T02:40:30.791759Z","iopub.execute_input":"2025-08-07T02:40:30.792329Z","execution_failed":"2025-08-07T04:29:45.464Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Starting to process benchmarks from /kaggle/working/ispd2005\n\nProcessing benchmark: bigblue4\nParsing nodes file...\nFound 2177353 unique nodes.\nParsing nets file...\nFound 2229886 nets.\nAdding edges based on net connections (clique model)...\nGraph created with 2177353 nodes and 162088542 edges.\nGraphBuilder initialized and graph built.\nCalculating Degree Centrality...\nCreated new benchmark folder: /kaggle/working/ispd2005_degree/bigblue4_degree\nSaved new .nodes file to: /kaggle/working/ispd2005_degree/bigblue4_degree/bigblue4.nodes\nCopied 9 other files.\n\nProcessing benchmark: adaptec4\nParsing nodes file...\nFound 496045 unique nodes.\nParsing nets file...\nFound 515951 nets.\nAdding edges based on net connections (clique model)...\nGraph created with 496045 nodes and 13093493 edges.\nGraphBuilder initialized and graph built.\nCalculating Degree Centrality...\nCreated new benchmark folder: /kaggle/working/ispd2005_degree/adaptec4_degree\nSaved new .nodes file to: /kaggle/working/ispd2005_degree/adaptec4_degree/adaptec4.nodes\nCopied 9 other files.\n\nProcessing benchmark: adaptec3\nParsing nodes file...\nFound 451650 unique nodes.\nParsing nets file...\nFound 466758 nets.\nAdding edges based on net connections (clique model)...\nGraph created with 451650 nodes and 11577111 edges.\nGraphBuilder initialized and graph built.\nCalculating Degree Centrality...\nCreated new benchmark folder: /kaggle/working/ispd2005_degree/adaptec3_degree\nSaved new .nodes file to: /kaggle/working/ispd2005_degree/adaptec3_degree/adaptec3.nodes\nCopied 9 other files.\n\nProcessing benchmark: bigblue2\nParsing nodes file...\nFound 557866 unique nodes.\nParsing nets file...\nFound 577235 nets.\nAdding edges based on net connections (clique model)...\nGraph created with 557866 nodes and 75873855 edges.\nGraphBuilder initialized and graph built.\nCalculating Degree Centrality...\nCreated new benchmark folder: /kaggle/working/ispd2005_degree/bigblue2_degree\nSaved new .nodes file to: /kaggle/working/ispd2005_degree/bigblue2_degree/bigblue2.nodes\nCopied 9 other files.\n\nProcessing benchmark: bigblue3\nParsing nodes file...\nFound 1096812 unique nodes.\nParsing nets file...\nFound 1123170 nets.\nAdding edges based on net connections (clique model)...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Testing BBO RS (In testing)","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/lamda-bbo/WireMask-BBO.git\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T09:20:11.741666Z","iopub.execute_input":"2025-08-08T09:20:11.741859Z","iopub.status.idle":"2025-08-08T09:20:12.557662Z","shell.execute_reply.started":"2025-08-08T09:20:11.741842Z","shell.execute_reply":"2025-08-08T09:20:12.556493Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'WireMask-BBO'...\nremote: Enumerating objects: 58, done.\u001b[K\nremote: Counting objects: 100% (1/1), done.\u001b[K\nremote: Total 58 (delta 0), reused 0 (delta 0), pack-reused 57 (from 1)\u001b[K\nReceiving objects: 100% (58/58), 142.85 KiB | 3.97 MiB/s, done.\nResolving deltas: 100% (7/7), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -r /kaggle/working/WireMask-BBO/requirements.txt\n!pip install pyunpack\n!pip install gpytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T09:20:12.559056Z","iopub.execute_input":"2025-08-08T09:20:12.559421Z","iopub.status.idle":"2025-08-08T09:21:08.443389Z","shell.execute_reply.started":"2025-08-08T09:20:12.559391Z","shell.execute_reply":"2025-08-08T09:21:08.442348Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Collecting gpytorch==1.8.1 (from -r /kaggle/working/WireMask-BBO/requirements.txt (line 1))\n  Downloading gpytorch-1.8.1-py2.py3-none-any.whl.metadata (6.8 kB)\nCollecting matplotlib==2.2.3 (from -r /kaggle/working/WireMask-BBO/requirements.txt (line 2))\n  Downloading matplotlib-2.2.3.tar.gz (36.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.8/36.8 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting numpy==1.15.1 (from -r /kaggle/working/WireMask-BBO/requirements.txt (line 3))\n  Downloading numpy-1.15.1.zip (4.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: Ignored the following yanked versions: 3.4.11.39, 3.4.17.61, 4.4.0.42, 4.4.0.44, 4.5.4.58, 4.5.5.62, 4.7.0.68\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement opencv_python==4.1.2.30 (from versions: 3.4.0.14, 3.4.10.37, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.15.55, 3.4.16.57, 3.4.16.59, 3.4.17.63, 3.4.18.65, 4.3.0.38, 4.4.0.40, 4.4.0.46, 4.5.1.48, 4.5.3.56, 4.5.4.60, 4.5.5.64, 4.6.0.66, 4.7.0.72, 4.8.0.74, 4.8.0.76, 4.8.1.78, 4.9.0.80, 4.10.0.82, 4.10.0.84, 4.11.0.86, 4.12.0.88)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for opencv_python==4.1.2.30\u001b[0m\u001b[31m\n\u001b[0mCollecting pyunpack\n  Downloading pyunpack-0.3-py2.py3-none-any.whl.metadata (863 bytes)\nCollecting easyprocess (from pyunpack)\n  Downloading EasyProcess-1.1-py3-none-any.whl.metadata (855 bytes)\nCollecting entrypoint2 (from pyunpack)\n  Downloading entrypoint2-1.1-py2.py3-none-any.whl.metadata (1.0 kB)\nDownloading pyunpack-0.3-py2.py3-none-any.whl (4.1 kB)\nDownloading EasyProcess-1.1-py3-none-any.whl (8.7 kB)\nDownloading entrypoint2-1.1-py2.py3-none-any.whl (9.9 kB)\nInstalling collected packages: entrypoint2, easyprocess, pyunpack\nSuccessfully installed easyprocess-1.1 entrypoint2-1.1 pyunpack-0.3\nCollecting gpytorch\n  Downloading gpytorch-1.14-py3-none-any.whl.metadata (8.0 kB)\nCollecting jaxtyping (from gpytorch)\n  Downloading jaxtyping-0.3.2-py3-none-any.whl.metadata (7.0 kB)\nRequirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.11/dist-packages (from gpytorch) (1.3.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from gpytorch) (1.2.2)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from gpytorch) (1.15.3)\nCollecting linear-operator>=0.6 (from gpytorch)\n  Downloading linear_operator-0.6-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from linear-operator>=0.6->gpytorch) (2.6.0+cu124)\nRequirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy>=1.6.0->gpytorch) (1.26.4)\nCollecting wadler-lindig>=0.1.3 (from jaxtyping->gpytorch)\n  Downloading wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->gpytorch) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->gpytorch) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy>=1.6.0->gpytorch) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy>=1.6.0->gpytorch) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy>=1.6.0->gpytorch) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy>=1.6.0->gpytorch) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy>=1.6.0->gpytorch) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy>=1.6.0->gpytorch) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0->linear-operator>=0.6->gpytorch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (1.13.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->linear-operator>=0.6->gpytorch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy>=1.6.0->gpytorch) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy>=1.6.0->gpytorch) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.5,>=1.23.5->scipy>=1.6.0->gpytorch) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.5,>=1.23.5->scipy>=1.6.0->gpytorch) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.5,>=1.23.5->scipy>=1.6.0->gpytorch) (2024.2.0)\nDownloading gpytorch-1.14-py3-none-any.whl (277 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.7/277.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading linear_operator-0.6-py3-none-any.whl (176 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.3/176.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jaxtyping-0.3.2-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m170.3/207.5 MB\u001b[0m \u001b[31m173.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading wadler_lindig-0.1.7-py3-none-any.whl (20 kB)\n\u001b[31mERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\n    unknown package:\n        Expected sha256 ea4f11a2904e2a8dc4b1833cc1b5181cde564edd0d5cd33e3c168eff2d1863f1\n             Got        7498753f7247a0c33bebc639a09ec39abf6e062c3b0613582316e10265cbbe53\n\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/working/WireMask-BBO')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T09:32:39.408923Z","iopub.execute_input":"2025-08-08T09:32:39.410064Z","iopub.status.idle":"2025-08-08T09:32:39.414145Z","shell.execute_reply.started":"2025-08-08T09:32:39.410028Z","shell.execute_reply":"2025-08-08T09:32:39.413338Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from place_db import PlaceDB\nfrom utils import random_guiding, greedy_placer_with_init_coordinate, write_final_placement, rank_macros\nfrom common import grid_setting, my_inf\nimport random\nimport time\nimport csv\nimport os\n\ndef BBO_RS(dataset, seed, stop_round):\n    \"\"\"\n    Runs a greedy placement algorithm with random initial guiding.\n\n    Args:\n        dataset (str): The name of the dataset to use.\n        seed (int): The random seed for reproducibility.\n        stop_round (int): The number of rounds to run the placement for.\n    \"\"\"\n    random.seed(seed)\n    placedb = PlaceDB(dataset)\n    \n    hpwl_save_dir = \"result/Random/curve/\"\n    placement_save_dir = \"result/Random/placement/\"\n    node_id_ls = rank_macros(placedb)\n\n    if not os.path.exists(hpwl_save_dir):\n        os.makedirs(hpwl_save_dir)\n    if not os.path.exists(placement_save_dir):\n        os.makedirs(placement_save_dir)\n\n    hpwl_save_dir += \"{}_seed_{}.csv\".format(dataset, seed)\n    placement_save_dir += \"{}_seed_{}.csv\".format(dataset, seed)\n    \n    hpwl_save_file = open(hpwl_save_dir, \"a+\")\n    hpwl_writer = csv.writer(hpwl_save_file)\n\n    grid_num = grid_setting[dataset][\"grid_num\"]\n    grid_size = grid_setting[dataset][\"grid_size\"]\n\n    best_hpwl = my_inf\n    best_placed_macro = None\n    \n    for _ in range(stop_round):\n        print(\"init\")\n        place_record = random_guiding(node_id_ls, placedb, grid_num, grid_size)\n        placed_macros, hpwl = greedy_placer_with_init_coordinate(node_id_ls, placedb, grid_num, grid_size, place_record)\n        \n        if hpwl < best_hpwl:\n            best_hpwl = hpwl\n            best_placed_macro = placed_macros\n            write_final_placement(best_placed_macro, placement_save_dir)\n        \n        hpwl_writer.writerow([hpwl, time.time(), \"init\"])\n        hpwl_save_file.flush()\n    \n    hpwl_save_file.close()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T09:33:02.208346Z","iopub.execute_input":"2025-08-08T09:33:02.209461Z","iopub.status.idle":"2025-08-08T09:33:02.865596Z","shell.execute_reply.started":"2025-08-08T09:33:02.209428Z","shell.execute_reply":"2025-08-08T09:33:02.864666Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"BBO_RS(dataset=\"adaptec1\", seed=42, stop_round=100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T09:33:07.244935Z","iopub.execute_input":"2025-08-08T09:33:07.245502Z","iopub.status.idle":"2025-08-08T09:33:07.353491Z","shell.execute_reply.started":"2025-08-08T09:33:07.245473Z","shell.execute_reply":"2025-08-08T09:33:07.352258Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/1031666893.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mBBO_RS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adaptec1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_37/1925858762.py\u001b[0m in \u001b[0;36mBBO_RS\u001b[0;34m(dataset, seed, stop_round)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \"\"\"\n\u001b[1;32m     18\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mplacedb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceDB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mhpwl_save_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"result/Random/curve/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/WireMask-BBO/place_db.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, benchmark)\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport_to_net_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_port_to_net_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"benchmark\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbenchmark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m             \u001b[0mnode_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"benchmark\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbenchmark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbenchmark\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".nodes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_info_raw_id_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_node_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbenchmark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: "],"ename":"AssertionError","evalue":"","output_type":"error"}],"execution_count":6}]}